{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from query.datasets.prelude import *\n",
    "import numpy\n",
    "import pysrt\n",
    "import textacy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcripts and compute time offsets for captions\n",
    "transcripts = []\n",
    "#for video in Video.objects.exclude(path__contains='segment'):\n",
    "#    base = os.path.split(os.path.splitext(video.path)[0])[1]\n",
    "     #subs = pysrt.open('/app/subs/{}.cc5.srt'.format(base))\n",
    "subs = pysrt.open('../data/videos/CNNW_20120201_160000_CNN_Newsroom.cc5.srt')\n",
    "offsets = []\n",
    "transcript = ''\n",
    "for sub in subs:\n",
    "    offsets.append((len(transcript), sub.start, sub.end))\n",
    "    transcript += sub.text.replace('\\n', ' ').replace('>', ' ') + ' '\n",
    "transcripts.append((None, transcript, offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[0].start\n",
    "subs[0].end\n",
    "subs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transcripts into textacy Docs (implicitly performing various NLP tasks)\n",
    "#docs = [textacy.doc.Doc(transcript, lang=u'en', metadata={'path': video.path, 'offsets': offsets}) for (video, transcript, offsets) in transcripts]\n",
    "docs = [textacy.doc.Doc(transcript, lang=u'en', metadata={'offsets': offsets}) for (video, transcript, offsets) in transcripts]\n",
    "corpus = textacy.Corpus(u'en', docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def token_time(doc, tok):\n",
    "    tok_offset = tok.idx\n",
    "    for i, (offset, start, end) in enumerate(doc.metadata['offsets']):\n",
    "        if tok_offset < offset:\n",
    "            return (start, end)\n",
    "    raise Exception(\"No token time?\")\n",
    "    \n",
    "def compute_tfidf(corpus):\n",
    "    gensim_dict = gensim.corpora.Dictionary(corpus)\n",
    "    gensim_corpus = [gensim_dict.doc2bow(text) for text in transcript_words]\n",
    "    tfidf_model = gensim.models.tfidfmodel.TfidfModel(gensim_corpus, normalize=True)\n",
    "    return {gensim_dict.get(id): value for doc in tfidf[gensim_corpus] for id, value in doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def sim_score(a, b):\n",
    "    try:\n",
    "        return model.wv.similarity(a, b)\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40456532973171394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('economy', 'jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Labeler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04a85c667755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtopic_labeler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mtopic_tracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Labeler' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: incorporate TFIDF segments\n",
    "\n",
    "topics = ['immigration', 'china', 'syria', 'terrorism', 'economy', 'election']\n",
    "transcript_tokens = [\n",
    "    list(textacy.extract.words(doc, filter_nums=True, include_pos=['PROPN', 'NOUN'])) \n",
    "    for doc in corpus.docs]\n",
    "  \n",
    "N = len(topics)    \n",
    "#fig, ax = plt.subplots(2, N / 2, figsize=(18, 16))\n",
    "#ax = ax.flatten()\n",
    "W = 30\n",
    "MENTION_THRESHOLD = 3\n",
    "PEAK_DURATION_THRESHOLD = 5\n",
    "\n",
    "def topic_peaks(topic, tokens):\n",
    "    scores = np.array([sim_score(token.text.lower(), topic) for token in tokens])\n",
    "    scores = scores > 0.6\n",
    "    sums = np.array([np.sum(scores[j:j+W]) for j in range(W, len(scores) - W)]) >= MENTION_THRESHOLD\n",
    "    start = None\n",
    "    peaks = []\n",
    "    for j in range(len(sums)):\n",
    "        if sums[j] and start is None:\n",
    "            start = j\n",
    "        elif not sums[j] and start is not None:\n",
    "            idx = (start + j) / 2 + W\n",
    "            peak_duration = j - start\n",
    "            if peak_duration > PEAK_DURATION_THRESHOLD:\n",
    "                peaks.append((tokens[idx], peak_duration))\n",
    "            start = None\n",
    "\n",
    "    #ax[i].set_ylim([0, 1])\n",
    "    #ax[i].set_title(topic)\n",
    "    #ax[i].plot(sums)\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "\n",
    "topic_labeler, _ = Labeler.objects.get_or_create(name='word2vec')\n",
    "topic_tracks = []\n",
    "for tokens, doc in zip(transcript_tokens, corpus.docs):\n",
    "    print(doc.metadata['path'])\n",
    "    video = Video.objects.get(path=doc.metadata['path'])\n",
    "    for i, topic in enumerate(topics): \n",
    "        peaks = topic_peaks(topic, tokens)\n",
    "        print(topic, peaks)\n",
    "        for (token, _) in peaks:\n",
    "            (start, end) = token_time(doc, token)\n",
    "            start = start.to_time()\n",
    "            seconds = (((start.hour * 60) + start.minute) * 60 + start.second)\n",
    "            \n",
    "            # Naively assume 4 minute segment size\n",
    "            min_time = max(seconds - 2 * 60, 0)\n",
    "            max_time = min(seconds + 2 * 60, int(video.num_frames / video.fps))\n",
    "            \n",
    "            topic_model, _ = Topic.objects.get_or_create(name=topic)\n",
    "            track = TopicTrack(\n",
    "                video=video, min_frame=min_time*video.fps, max_frame=max_time*video.fps, topic=topic_model,\n",
    "                labeler=topic_labeler)\n",
    "            topic_tracks.append(track)\n",
    "#_ = TopicTrack.objects.bulk_create(topic_tracks)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
